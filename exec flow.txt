train_smac: CENTRALIZED
	- initilize arguments (map: 3m)
	- create environment
	- create RUNNER:
		- create 1 ACTOR CRITIC (wrapper)
        - create 1 BUFFER 

        - start training 
            - for each eps:
                - collect data (run policy network)
                - run env (with collected actions)
                - insert data to BUFFER
                - compute returns()
                - update network(train network)

                -post process


train_smac: CENTRALIZED
	- initilize arguments (map: 3m)
	- create environment
	- create RUNNER:
		- create ACTOR CRITIC (wrapper) based on the number of units taken from map info
        - create BUFFER based on the number of units

        - start training 

            - for each eps:
            
                - for each agent in agent_id:
                    - collect individual data (run policy network of each agent)

                - run env (with collected actions data)

                - for each agent in agent_id:
                    - insert individual data to corespond individual BUFFER
                    - insert step data to each coresponding BUFFER
                    - compute returns for each BUFFER
                    - update individual network(train network)

                -post process

BUFFER(state, action, nextState, reward)

note: use share observations
- each agent_id has a SEPARATED RUNNER, BUFFER (check to see if buffer needed to be rewrite for decentralize  training) 
    FIND OUT WHAT NEED TO BE USED IN THE SEPARATED BUFFER
    base_runner
    smac_runner (REMEMBER to add lr_decay for each agent_id)
- tune arguments for decentralized
    share_policy
    use_ventralizedV
- use original return(long term reward) 
- check reward logic

- FIND OUT WHAT IS action_env (MPE exclusive)

env.step: tell all env to take the action. Whats the point of multithreading??